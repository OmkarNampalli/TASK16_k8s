# TASK16_k8s
This task is all about the DEPLOYMENT in kubernetes
```
Deployment â†’ manages â†’ ReplicaSet â†’ manages â†’ Pods â†’ run â†’ Containers
```
---

# Deployment
**Deployment is a controler object of kubernetes which manages set of identical pods and ensures they run reliably in your cluster**
## ğŸ”¹ What a Deployment Does
A Deployment:
1. Creates Pods (via a ReplicaSet)
2. Keeps the desired number of Pods running
3. Handles rolling updates
4. Supports rollbacks to previous versions
5. Manages scaling (up/down)

---

## Replica Sets
A **ReplicaSet** ensures that a specified number of identical Pods are running at all times.
In simple words:
> Keep N copies of this Pod always running.
### ğŸ”¹ Why Do We Need ReplicaSet?
Problem without ReplicaSet:
- Pod crashes â†’ not recreated
- Node dies â†’ Pod lost
- No scaling

ReplicaSet solves this by:
- Monitoring Pods
- Recreating failed Pods
- Maintaining desired count

#### ğŸ”¥ Real Example
If you define:
```
replicas: 3
```
ReplicaSet ensures:
- Always 3 Pods running
- If 1 crashes â†’ new one created
- If you scale to 5 â†’ 2 more created
- If you scale down to 2 â†’ 3 removed

### ğŸ”¥ What Happens Internally?
ReplicaSet Controller continuously watches:
- Current Pods count
- Desired replicas

If current < desired â†’ create Pod
If current > desired â†’ delete Pod

This is done via Kubernetes control loop.

Deployment internally creates ReplicaSets.

When you update image:

- Deployment creates NEW ReplicaSet
- Gradually scales down old RS
- Scales up new RS

Thatâ€™s **rolling update**.

> ReplicaSet is a Kubernetes controller that ensures a specified number of pod replicas are running at all times. It maintains desired state by creating or deleting pods as necessary. Deployments use ReplicaSets internally to manage scaling and rolling updates.


### ğŸ”¥ Advanced Insight
ReplicaSet works using:

Kubernetes **reconciliation loop**.

Control plane continuously checks:

Desired state (in etcd)
vs
Current state (running pods)

If mismatch â†’ corrective action.

---

## Rolling Update (rollout)
Rollout & Rollback are handled by Deployment, but internally they work using ReplicaSets.

Let's say you have an **update for your app** and you have to **deploy it** in kubernetes.
But you **don't want any downtime** for your app.
To solve the above problems we use **rollling update (rollout)**.
Basically we provide latest or updated container to **deployment object**.
Then Deployment:
- Creates NEW ReplicaSet v2
- Gradually increases replicas of v2
- Gradually decreases replicas of v1
- Ensures availability during transition

In one line
> Rollout = Updating application without downtime.

### ğŸ”¥ Rolling Update Strategy
Default strategy:
```
strategy:
  type: RollingUpdate
  rollingUpdate:
    maxSurge: 1
    maxUnavailable: 1
```

#### ğŸ”¹ maxSurge

How many extra pods can be created during update.

Example:
replicas = 3
maxSurge = 1

Kubernetes can temporarily create 4 pods.

#### ğŸ”¹ maxUnavailable

How many pods can be unavailable during update.

Example:
maxUnavailable = 1

Only 1 pod can be down at a time.

### ğŸ”¥ How To Trigger Rollout
#### Method 1: Update Image
```
kubectl set image deployment/nginx-deployment nginx=nginx:1.22
```

This triggers rollout.

#### Method 2: Edit Deployment
```
kubectl edit deployment nginx-deployment
```

Change image manually.

#### Method 3: Apply Updated YAML
```
kubectl apply -f deployment.yaml
```

If spec changes â†’ rollout starts.

#### ğŸ”¥ Check Rollout Status
```
kubectl rollout status deployment nginx-deployment
```
#### ğŸ”¥ See Rollout History
```
kubectl rollout history deployment nginx-deployment
```

#### ğŸ”¥ Pause & Resume Rollout

Pause:
```
kubectl rollout pause deployment nginx-deployment
```
Resume:
```` Bash
kubectl rollout resume deployment nginx-deployment
````
Used in controlled releases.

---

## Rollout


Now assume you update your app using rollout.
And something goes wrong in your application (bugs, app cannot reach DB, etc) but your **previous version** was working perfectly so you want to deploy previous version with zero downtime.

To perform this operation we use **rollback**
Deployment keeps old ReplicaSets.

When you rollback:

1. Deployment scales UP old ReplicaSet
2. Scales DOWN current faulty ReplicaSet
3. Updates revision number

It does NOT rebuild from scratch.

It just switches ReplicaSets.
### ğŸ”¥ Rollback Command
Rollback to previous revision:
```
kubectl rollout undo deployment nginx-deployment
```
Rollback to specific revision:
```
kubectl rollout undo deployment nginx-deployment --to-revision=1
```

### ğŸ”¥ What Happens Internally During Rollback

Current State:

- ReplicaSet v1 â†’ 0 pods
- ReplicaSet v2 â†’ 3 pods

After rollback:

- ReplicaSet v1 â†’ 3 pods
- ReplicaSet v2 â†’ 0 pods

Same rolling strategy rules apply.

## ğŸ”¥ Full Real Flow (CI/CD)
```
Developer pushes code
â†“
Docker image built
â†“
Image pushed to registry
â†“
Deployment YAML updated
â†“
kubectl apply
â†“
New ReplicaSet created
â†“
Rolling update happens
â†“
Old ReplicaSet scaled down
â†“
Deployment successful

If failure:
kubectl rollout undo
```

Q. â€œHow does Kubernetes perform rolling updates?â€

Answer:

Deployment creates a new ReplicaSet with updated pod template and gradually scales it up while scaling down the old ReplicaSet according to maxSurge and maxUnavailable settings, ensuring zero downtime.

---

## LivenessProbe and ReadinessProbe
**ğŸ”¥ Why Do We Need Probes?**

Imagine:
Your container is running
BUT your app inside is crashed

From Kubernetes point of view:
- Pod = Running
- Container = Running

But your app is broken.

Without probes â†’ Kubernetes wonâ€™t know.

Probes allow Kubernetes to:

- Detect broken app
- Restart container
Stop traffic to unhealthy pods
### livenessProbe
#### ğŸ”¹ What is Liveness Probe?

Checks:
> â€œIs the container alive?â€

If it fails â†’ Kubernetes restarts the container.

#### ğŸ”¥ What Happens Internally?

If liveness probe fails:

Kubelet:
* 1ï¸âƒ£ Kills container
* 2ï¸âƒ£ Restarts container
* 3ï¸âƒ£ Pod stays same

It does NOT recreate Pod.

It restarts container inside Pod.

### ğŸ”¥ 2ï¸âƒ£ Readiness Probe
#### ğŸ”¹ What is Readiness Probe?

Checks:
> â€œIs this pod ready to serve traffic?â€

If it fails:
* Pod stays running
* BUT traffic is stopped

Service removes pod from endpoints.

#### ğŸ”¥ What Happens Internally?

If readiness probe fails:
* 1ï¸âƒ£ Pod remains running
* 2ï¸âƒ£ Service removes Pod IP from endpoints
* 3ï¸âƒ£ No traffic routed

If readiness passes again:

Pod added back to Service endpoints.

| Feature         | Liveness           | Readiness       |
| --------------- | ------------------ | --------------- |
| Purpose         | Restart container  | Stop traffic    |
| If fails        | Container restarts | Traffic removed |
| Affects Service | No                 | Yes             |

---

| Feature                      | Pod             | ReplicaSet                  | Deployment                  |
| ---------------------------- | --------------- | --------------------------- | --------------------------- |
| What it is                   | Smallest unit   | Pod manager                 | ReplicaSet manager          |
| Purpose                      | Runs containers | Maintains desired Pod count | Manages updates & lifecycle |
| Self-healing                 | âŒ No            | âœ… Yes                       | âœ… Yes                       |
| Scaling                      | âŒ Manual        | âœ… Yes                       | âœ… Yes                       |
| Rolling updates              | âŒ No            | âŒ No                        | âœ… Yes                       |
| Rollback support             | âŒ No            | âŒ No                        | âœ… Yes                       |
| Used directly in production? | Rarely          | Rarely                      | âœ… Yes                       |

## ğŸ¯ One-Line Summary

* **Pod** = Runs containers
* **ReplicaSet** = Keeps Pods alive
* **Deployment** = Manages updates and scaling safely
